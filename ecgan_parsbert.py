# -*- coding: utf-8 -*-
"""ECGAN_parsBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVXvvWwhNoz82-NVC-rw2r4x9gRNw7jh
"""

# !pip install transformers==4.3.2
from genericpath import exists
import glob
import torch
import os
import io
import torch.nn.functional as F
import random
import numpy as np
import pandas as pd
import time
import math
import datetime
import torch.nn as nn
from transformers import *
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html
#!pip install sentencepiece


##########################
# GET PARAMS WITH SWITCH
##########################
import getopt
import sys
argumentList = sys.argv[1:]
# Options
options = "hd:p:w:t:e:"
# Long options
long_options = ["help", "dataset", "percentage", "weight", "thresh","epochs"]

dataset_name = "persiannews"
percentage_labeled_data = 0.1
adversarial_weight = 0.1
confidence_thresh = 0.2
num_epochs = 10
try:
    # Parsing argument
    arguments, values = getopt.getopt(argumentList, options, long_options)
    # checking each argument
    for currentArgument, currentValue in arguments:
        if currentArgument in ("-h", "--help"):
            print("Displaying Help")
        elif currentArgument in ("-d", "--dataset"):
            dataset_name = currentValue
        elif currentArgument in ("-p", "--percentage"):
            percentage_labeled_data = float(currentValue)
        elif currentArgument in ("-w", "--weight"):
            adversarial_weight = float(currentValue)
        elif currentArgument in ("-t", "--thresh"):
            confidence_thresh = float(currentValue)
        elif currentArgument in ("-e", "--epochs"):
            num_epochs = int(currentValue)
except getopt.error as err:
    # output error, and return with an error code
    print(str(err))

print("Dataset : {} \nPercentage : {}".format(
    dataset_name, percentage_labeled_data))


# Set random values
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed_val)

# If there's a GPU available...
if torch.cuda.is_available():
    # Tell PyTorch to use the GPU.
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""### Input Parameters

"""


dataSizeConstant = percentage_labeled_data

# --------------------------------
#  Transformer parameters
# --------------------------------
max_seq_length = 128
batch_size = 32


# --------------------------------
#  GAN-BERT specific parameters
# --------------------------------
# number of hidden layers in the generator,
# each of the size of the output space
num_hidden_layers_g = 1
# number of hidden layers in the discriminator,
# each of the size of the input space
num_hidden_layers_d = 1

num_hidden_layers_c = 1

# size of the generator's input noisy vectors
noise_size = 100
# dropout to be applied to discriminator's input vectors
out_dropout_rate = 0.2

# Replicate labeled data to balance poorly represented datasets,
# e.g., less than 1% of labeled material
apply_balance = True

# --------------------------------
#  Optimization parameters
# --------------------------------
learning_rate_discriminator = 5e-5
learning_rate_generator = 5e-5
learning_rate_classifier = 5e-5
epsilon = 1e-8

num_train_epochs = num_epochs

multi_gpu = True
# Scheduler
apply_scheduler = False
warmup_proportion = 0.1
# Print
print_each_n_step = 10

#model_name = "HooshvareLab/bert-fa-base-uncased"
# @param ["HooshvareLab/bert-fa-base-uncased","HooshvareLab/bert-fa-zwnj-base"] {allow-input: true}
model_name = 'HooshvareLab/bert-fa-base-uncased'

dataset = dataset_name
cmd_str = "git clone https://github.com/iamardian/{}.git".format(dataset)
os.system(cmd_str)
labeled_file = "./{}/train.csv".format(dataset)
validation_file = "./{}/dev.csv".format(dataset)
test_filename = "./{}/test.csv".format(dataset)

print(labeled_file)
print(validation_file)
print(test_filename)

"""Load the Tranformer Model"""

transformer = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""Function required to load the dataset"""

df = pd.read_csv(labeled_file, sep='\t')
lst_cnt = df.label_id.to_list()
label_list = list(set(lst_cnt))
print(label_list)


def get_qc_examples(input_file):
    """Creates examples for the training and dev sets."""

    df = pd.read_csv(input_file, sep='\t')
    lst_cnt = df.content.to_list()
    lst_id = df.label_id.to_list()
    examples = list(zip(lst_cnt, lst_id))
    return examples


"""Load the PersianNews and Digimag Datasets then separate labeled and unlabeled data."""

# Load the examples
labeled_examples = get_qc_examples(labeled_file)
subset = np.random.permutation([i for i in range(len(labeled_examples))])
number_of_sample = subset[:int(len(labeled_examples) * (dataSizeConstant))]
labeled_examples = [labeled_examples[i] for i in number_of_sample]
validation_examples = get_qc_examples(validation_file)
test_examples = get_qc_examples(test_filename)

"""Functions required to convert examples into Dataloader"""


def generate_data_loader(input_examples, label_masks, label_map, do_shuffle=False, balance_label_examples=False):
    '''
    Generate a Dataloader given the input examples, eventually masked if they are 
    to be considered NOT labeled.
    '''
    examples = []

    # Count the percentage of labeled examples
    num_labeled_examples = 0
    for label_mask in label_masks:
        if label_mask:
            num_labeled_examples += 1
    label_mask_rate = num_labeled_examples/len(input_examples)

    # if required it applies the balance
    for index, ex in enumerate(input_examples):
        if label_mask_rate == 1 or not balance_label_examples:
            examples.append((ex, label_masks[index]))
        else:
            # IT SIMULATE A LABELED EXAMPLE
            if label_masks[index]:
                balance = int(1/label_mask_rate)
                balance = int(math.log(balance, 2))
                if balance < 1:
                    balance = 1
                for b in range(0, int(balance)):
                    examples.append((ex, label_masks[index]))
            else:
                examples.append((ex, label_masks[index]))

    # -----------------------------------------------
    # Generate input examples to the Transformer
    # -----------------------------------------------
    input_ids = []
    input_mask_array = []
    label_mask_array = []
    label_id_array = []

    # Tokenization
    for (text, label_mask) in examples:
        encoded_sent = tokenizer.encode(
            text[0], add_special_tokens=True, max_length=max_seq_length, padding="max_length", truncation=True)
        input_ids.append(encoded_sent)
        label_id_array.append(label_map[str(text[1])])
        label_mask_array.append(label_mask)

    # Attention to token (to ignore padded input wordpieces)
    for sent in input_ids:
        att_mask = [int(token_id > 0) for token_id in sent]
        input_mask_array.append(att_mask)
    # Convertion to Tensor
    input_ids = torch.tensor(input_ids)
    input_mask_array = torch.tensor(input_mask_array)
    label_id_array = torch.tensor(label_id_array, dtype=torch.long)
    label_mask_array = torch.tensor(label_mask_array)

    # Building the TensorDataset
    dataset = TensorDataset(input_ids, input_mask_array,
                            label_id_array, label_mask_array)

    if do_shuffle:
        sampler = RandomSampler
    else:
        sampler = SequentialSampler

    # Building the DataLoader
    return DataLoader(
        dataset,  # The training samples.
        sampler=sampler(dataset),
        batch_size=batch_size)  # Trains with this batch size.


def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))


"""Convert the input examples into DataLoader"""

label_map = {}
for (i, label) in enumerate(label_list):
    label_map[str(label)] = i
# ------------------------------
#   Load the train dataset
# ------------------------------
train_examples = labeled_examples
# The labeled (train) dataset is assigned with a mask set to True
train_label_masks = np.ones(len(labeled_examples), dtype=bool)
train_dataloader = generate_data_loader(
    train_examples, train_label_masks, label_map, do_shuffle=True, balance_label_examples=apply_balance)


validation_label_masks = np.ones(len(validation_examples), dtype=bool)
validation_dataloader = generate_data_loader(
    validation_examples, validation_label_masks, label_map, do_shuffle=True, balance_label_examples=apply_balance)


# ------------------------------
#   Load the test dataset
# ------------------------------
# The labeled (test) dataset is assigned with a mask set to True
test_label_masks = np.ones(len(test_examples), dtype=bool)
test_dataloader = generate_data_loader(
    test_examples, test_label_masks, label_map, do_shuffle=False, balance_label_examples=False)

"""We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"""

# ------------------------------
#   The Generator as in
#   https://www.aclweb.org/anthology/2020.acl-main.191/
#   https://github.com/crux82/ganbert
# ------------------------------


class Generator(nn.Module):
    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):
        super(Generator, self).__init__()
        layers = []
        hidden_sizes = [noise_size] + hidden_sizes
        for i in range(len(hidden_sizes)-1):
            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),
                          nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])

        layers.append(nn.Linear(hidden_sizes[-1], output_size))
        self.layers = nn.Sequential(*layers)

    def forward(self, noise):
        output_rep = self.layers(noise)
        return output_rep
# ------------------------------
#   The Discriminator
#   https://www.aclweb.org/anthology/2020.acl-main.191/
#   https://github.com/crux82/ganbert
# ------------------------------


class Discriminator(nn.Module):
    def __init__(self, input_size=512, hidden_sizes=[512], dropout_rate=0.1):
        super(Discriminator, self).__init__()
        self.input_dropout = nn.Dropout(p=dropout_rate)
        layers = []
        hidden_sizes = [input_size] + hidden_sizes
        for i in range(len(hidden_sizes)-1):
            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),
                          nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])

        self.layers = nn.Sequential(*layers)  # per il flatten
        self.logit = nn.Linear(hidden_sizes[-1], 1)  # fake/real.
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_rep):
        input_rep = self.input_dropout(input_rep)
        last_rep = self.layers(input_rep)
        logits = self.logit(last_rep)
        output = self.sigmoid(logits)
        return output.view(-1, 1).squeeze(1)


class Classifier(nn.Module):
    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):
        super(Classifier, self).__init__()
        self.input_dropout = nn.Dropout(p=dropout_rate)
        layers = []
        hidden_sizes = [input_size] + hidden_sizes
        for i in range(len(hidden_sizes)-1):
            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),
                          nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])

        self.layers = nn.Sequential(*layers)  # per il flatten
        self.logit = nn.Linear(hidden_sizes[-1], num_labels)  # fake/real.
        self.softmax = nn.Softmax()

    def forward(self, input_rep):
        # input_rep = self.input_dropout(input_rep)
        last_rep = self.layers(input_rep)
        logits = self.logit(last_rep)
        output = self.softmax(logits)
        return output


"""We instantiate the Discriminator and Generator"""

# The config file is required to get the dimension of the vector produced by
# the underlying transformer
config = AutoConfig.from_pretrained(model_name)
hidden_size = int(config.hidden_size)
# Define the number and width of hidden layers
hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]
hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]
hidden_levels_c = [hidden_size for i in range(0, num_hidden_layers_c)]

# -------------------------------------------------
#   Instantiate the Generator and Discriminator
# -------------------------------------------------
generator = Generator(noise_size=noise_size, output_size=hidden_size,
                      hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)
discriminator = Discriminator(
    input_size=hidden_size, hidden_sizes=hidden_levels_d, dropout_rate=out_dropout_rate)
classifier = Classifier(input_size=hidden_size, hidden_sizes=hidden_levels_c,
                        num_labels=len(label_list), dropout_rate=out_dropout_rate)

# Put everything in the GPU if available
if torch.cuda.is_available():
    generator.cuda()
    discriminator.cuda()
    classifier.cuda()
    transformer.cuda()
    if multi_gpu:
        transformer = torch.nn.DataParallel(transformer)


# data for plotting purposes
generatorLosses = []
discriminatorLosses = []
classifierLosses = []

# models parameters
transformer_vars = [i for i in transformer.parameters()]
d_vars = [v for v in discriminator.parameters()]
c_vars = transformer_vars + [v for v in classifier.parameters()]
# c_vars = [v for v in classifier.parameters()]
g_vars = [v for v in generator.parameters()]

# optimizer
dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)
cfr_optimizer = torch.optim.AdamW(c_vars, lr=learning_rate_classifier)
gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator)

advWeight = adversarial_weight  # adversarial weight
offset = -1

loss = nn.BCELoss()
criterion = nn.CrossEntropyLoss()

total_acc_validation = []
total_acc_evaluation = []

best_model_accuracy = 0
default_path_str = "/content/drive/MyDrive/NLP/save/"
dir_name = f"ec_gan|{dataset_name}|{percentage_labeled_data}|{adversarial_weight}|{confidence_thresh}"
models_path = os.path.join(default_path_str, dir_name)
best_model_name = "best_model"


def load_best_model(load_path):
    print("call load_best_model")
    best_model_path = load_path + "/" + f"{best_model_name}.pth"
    checkpoint = torch.load(best_model_path)
    transformer = AutoModel.from_pretrained(model_name)
    classifier = Classifier(input_size=hidden_size, hidden_sizes=hidden_levels_c,
                            num_labels=len(label_list), dropout_rate=out_dropout_rate)
    transformer.load_state_dict(checkpoint['transformer_state_dict'])
    classifier.load_state_dict(checkpoint['classifier_state_dict'])
    return transformer, classifier


def save_best_model(save_path, epoch, accuracy):
    print("call save_best_model")
    create_path_if_not_exists(save_path)
    if best_model_accuracy >= accuracy:
        return
    best_model_path = os.path.join(save_path, f"{best_model_name}.pth")
    torch.save({
        'epoch': epoch,
        'transformer_state_dict': transformer.state_dict(),
        'classifier_state_dict': classifier.state_dict(),
    }, best_model_path)
    print("Best model Saved")


def print_validation_accuracy(index, acc):
    print("call print_validation_accuracy")
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        title = ["epoch", "acc"]
        total_acc_validation.append([index, acc])
        tdf = pd.DataFrame(total_acc_validation, columns=title)
        print("validation")
        print(tdf)
        print("validation")


def print_evaluation_accuracy(index, acc):
    print("call print_evaluation_accuracy")
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        title = ["epoch", "acc"]
        total_acc_evaluation.append([index, acc])
        tdf = pd.DataFrame(total_acc_evaluation, columns=title)
        print("evaluation")
        print(tdf)
        print("evaluation")


def find_latest_model_name(dir_path):
    print("call find_latest_model_name")
    model_name = ""
    if not os.path.exists(dir_path):
        return model_name
    files = sorted(filter(os.path.isfile, glob.glob(dir_path + '/*')))
    if len(files) == 0:
        return model_name
    for f in reversed(range(len(files))):
          if "best" in files[f]:
                continue
          else:
                model_name = files[f]
                break
    return model_name


def load_params(load_path,classifier,generator,discriminator,transformer,cfr_optimizer,gen_optimizer,dis_optimizer):
    print("call load_params")
    if not os.path.exists(load_path):
        print("not exists path : ", load_path)
        return
    model_path = find_latest_model_name(load_path)
    print(f"latest model : {model_path}")
    if model_path == "":
        return
    print(f"model_path : {model_path}")
    checkpoint = torch.load(model_path)
    global offset, best_model_accuracy, generatorLosses, discriminatorLosses, classifierLosses, total_acc_validation, total_acc_evaluation
    offset = checkpoint['epoch']
    print(f"offset : {offset}")
    best_model_accuracy = checkpoint['best_model_accuracy']

    transformer.load_state_dict(checkpoint['transformer_state_dict'])

    classifier.load_state_dict(checkpoint['classifier_state_dict'])
    cfr_optimizer.load_state_dict(checkpoint['cfr_optimizer_state_dict'])

    generator.load_state_dict(checkpoint['generator_state_dict'])
    gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])

    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
    dis_optimizer.load_state_dict(checkpoint['dis_optimizer_state_dict'])

    generatorLosses = checkpoint['generatorLosses']
    discriminatorLosses = checkpoint['discriminatorLosses']
    classifierLosses = checkpoint['classifierLosses']

    total_acc_validation = checkpoint['total_acc_validation']
    total_acc_evaluation = checkpoint['total_acc_evaluation']


def create_path_if_not_exists(dir_path):
    print("call create_path_if_not_exists")
    print(f"dir_path : {dir_path}")
    if os.path.exists(dir_path):
        return
    os.makedirs(dir_path, exist_ok=True)


def save_params(epoch, save_path):
    print("call save_params")
    try:
        create_path_if_not_exists(save_path)
        model_name_path = f'{str(epoch).zfill(3)}_{dataset_name}_{percentage_labeled_data}_{adversarial_weight}_{confidence_thresh}.pth'
        model_path_name = os.path.join(save_path, model_name_path)
        torch.save({
            'epoch': epoch,
            'best_model_accuracy': best_model_accuracy,

            'transformer_state_dict': transformer.state_dict(),

            'classifier_state_dict': classifier.state_dict(),
            'cfr_optimizer_state_dict': cfr_optimizer.state_dict(),

            'generator_state_dict': generator.state_dict(),
            'gen_optimizer_state_dict': gen_optimizer.state_dict(),

            'discriminator_state_dict': discriminator.state_dict(),
            'dis_optimizer_state_dict': dis_optimizer.state_dict(),

            'generatorLosses': generatorLosses,
            'discriminatorLosses': discriminatorLosses,
            'classifierLosses': classifierLosses,

            'total_acc_validation': total_acc_validation,
            'total_acc_evaluation': total_acc_evaluation,
        }, model_path_name)
        remove_previous_models(save_path, model_name_path)
    except:
        print("save model failed ...")


def remove_previous_models(dir_path, current_model):
    filelist = sorted(filter(os.path.isfile, glob.glob(dir_path + '/*')))
    print(filelist)
    for f in filelist:
        if (not ("best" in f) and not (current_model in f)):
            os.remove(os.path.join(dir_path, f))


def train(datasetloader):
    print("Training Start : ")
    load_params(models_path,classifier,generator,discriminator,transformer,cfr_optimizer,gen_optimizer,dis_optimizer)
    for epoch_i in range(offset+1, num_train_epochs):

        classifier.train()

        # ========================================
        #               Training
        # ========================================
        # Perform one full pass over the training set.
        print("")
        print(
            '======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))
        print('Training...')

        # Measure how long the training epoch takes.
        t0 = time.time()

        running_loss = 0.0
        total_train = 0
        correct_train = 0
        for i, batch in enumerate(datasetloader, 0):

            # Progress update every print_each_n_step batches.
            if i % print_each_n_step == 0 and not i == 0:
                # Calculate elapsed time in minutes.
                elapsed = format_time(time.time() - t0)
                # Report progress.
                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(
                    i, len(datasetloader), elapsed))

            # Unpack this training batch from our dataloader.
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)
            b_label_mask = batch[3].to(device)

            tmpBatchSize = b_input_ids.shape[0]

            # create label arrays
            true_label = torch.ones(tmpBatchSize, device=device)
            fake_label = torch.zeros(tmpBatchSize, device=device)

            noise = torch.zeros(tmpBatchSize, noise_size,
                                device=device).uniform_(0, 1)
            fakeImageBatch = generator(noise)

            real_cpu = batch[0].to(device)
            batch_size = real_cpu.size(0)

            # Encode real data in the Transformer
            model_outputs = transformer(
                b_input_ids, attention_mask=b_input_mask)
            hidden_states = model_outputs[-1]

            # train discriminator on real images
            predictionsReal = discriminator(hidden_states)
            lossDiscriminator = loss(predictionsReal, true_label)  # labels = 1
            lossDiscriminator.backward(retain_graph=True)

            # train discriminator on fake images
            predictionsFake = discriminator(fakeImageBatch)
            lossFake = loss(predictionsFake, fake_label)  # labels = 0
            lossFake.backward(retain_graph=True)
            dis_optimizer.step()  # update discriminator parameters

            # train generator
            gen_optimizer.zero_grad()
            predictionsFake = discriminator(fakeImageBatch)
            lossGenerator = loss(predictionsFake, true_label)  # labels = 1
            lossGenerator.backward(retain_graph=True)
            gen_optimizer.step()

            torch.autograd.set_detect_anomaly(True)
            fakeImageBatch = fakeImageBatch.detach().clone()

            # train classifier on real data
            predictions = classifier(hidden_states)
            realClassifierLoss = criterion(predictions, b_labels)
            realClassifierLoss.backward(retain_graph=True)

            cfr_optimizer.step()
            cfr_optimizer.zero_grad()

            # update the classifer on fake data
            predictionsFake = classifier(fakeImageBatch)
            # get a tensor of the labels that are most likely according to model
            predictedLabels = torch.argmax(
                predictionsFake, 1)  # -> [0 , 5, 9, 3, ...]
            confidenceThresh = confidence_thresh

            # psuedo labeling threshold
            probs = F.softmax(predictionsFake, dim=1)
            mostLikelyProbs = np.asarray(
                [probs[i, predictedLabels[i]].item() for i in range(len(probs))])
            toKeep = mostLikelyProbs > confidenceThresh
            if sum(toKeep) != 0:
                fakeClassifierLoss = criterion(
                    predictionsFake[toKeep], predictedLabels[toKeep]) * advWeight
                fakeClassifierLoss.backward()

            cfr_optimizer.step()

            # reset the gradients
            dis_optimizer.zero_grad()
            gen_optimizer.zero_grad()
            cfr_optimizer.zero_grad()

            # save losses for graphing
            generatorLosses.append(lossGenerator.item())
            discriminatorLosses.append(lossDiscriminator.item())
            classifierLosses.append(realClassifierLoss.item())

            # get train accurcy
            # if((i+1) % 10 == 0 ):
            #   classifier.eval()
            #   # accuracy
            #   _, predicted = torch.max(predictions, 1)
            #   total_train += b_labels.size(0)
            #   correct_train += predicted.eq(b_labels.data).sum().item()
            #   train_accuracy = 100 * correct_train / total_train
            #   print("({}) train_accuracy : {}".format(i+1,train_accuracy))
            #   classifier.train()

        print("Epoch " + str(epoch_i+1) + " Complete")
        evaluation(epoch_i)
        validation_acc = validate(epoch_i)
        save_best_model(models_path, epoch_i, validation_acc)
        save_params(epoch_i, models_path)


def validate(epoch):
    print("call validate")
    classifier.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for data in validation_dataloader:

            b_input_ids = data[0].to(device)
            b_input_mask = data[1].to(device)
            b_labels = data[2].to(device)

            # inputs, labels = data
            # inputs, labels = data[0].to(device), data[1].to(device)

            model_outputs = transformer(
                b_input_ids, attention_mask=b_input_mask)
            hidden_states = model_outputs[-1]

            outputs = classifier(hidden_states)
            _, predicted = torch.max(outputs.data, 1)
            total += b_labels.size(0)
            correct += (predicted == b_labels).sum().item()

    accuracy = (correct / total) * 100
    print_validation_accuracy(epoch+1, accuracy)
    print("validate : {} / {} * 100 = {} ".format(correct, total, accuracy))
    classifier.train()
    return accuracy


def evaluation(epoch):
    print("call evaluation")

    classifier.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for data in train_dataloader:

            b_input_ids = data[0].to(device)
            b_input_mask = data[1].to(device)
            b_labels = data[2].to(device)
            model_outputs = transformer(
                b_input_ids, attention_mask=b_input_mask)
            hidden_states = model_outputs[-1]

            outputs = classifier(hidden_states)
            _, predicted = torch.max(outputs.data, 1)
            total += b_labels.size(0)
            correct += (predicted == b_labels).sum().item()

    accuracy = (correct / total) * 100
    print_evaluation_accuracy(epoch+1, accuracy)
    print("evaluation : {} / {} * 100 = {} ".format(correct, total, accuracy))
    classifier.train()
    return accuracy


def test(transformer, classifier):
    print("call test")
    classifier.eval()

    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_dataloader:

            b_input_ids = data[0].to(device)
            b_input_mask = data[1].to(device)
            b_labels = data[2].to(device)
            model_outputs = transformer(
                b_input_ids, attention_mask=b_input_mask)
            hidden_states = model_outputs[-1]

            outputs = classifier(hidden_states)
            _, predicted = torch.max(outputs.data, 1)
            total += b_labels.size(0)
            correct += (predicted == b_labels).sum().item()

    accuracy = (correct / total) * 100
    print_evaluation_accuracy(accuracy)
    print("test : {} / {} * 100 = {} ".format(correct, total, accuracy))
    return accuracy


train(train_dataloader)


transformer, classifier = load_best_model(models_path)
test(transformer, classifier)
